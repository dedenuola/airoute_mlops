# RouteAQ â€“ End-to-End Air-Quality Forecasting MLOps Pipeline

Iâ€™m building **RouteAQ**, a one-hour-ahead (PMâ‚‚.â‚…,NOâ‚‚, Oâ‚ƒ) forecasting service for UK postcodes. At a high level, Iâ€™ve completed:

1. **Data Ingestion**  
   - **DEFRA AURN** hourly pollutant CSVs (PMâ‚‚.â‚…, NOâ‚‚, Oâ‚ƒ) landed daily via Airflow DAGs  
   - **Met Office** hourly forecast JSONs (temperature, wind, humidity) landed via a second Airflow task

2. **Feature Store**  
   - Defined a Feast repo with `feature_store.yaml` (local offline store & SQLite online store)  
   - Created a `FeatureView` (`aq_hourly`) that exposes tâ€“1 pollutant lags + latest weather

3. **Data Joining & Silver Tables**  
   - Extracted raw JSON & CSV into Parquet in `/data/silver`  
   - Joined pollutant + weather features on `(site_id, timestamp)` to produce a single training table

4. **Model Training & Registry**  
   - Trained a LightGBM regressor on Juneâ€“July data  
   - Logged parameters, metrics, and **model signature + input example** to MLflow  
   - Registered `routeaq_pm25` model (versions 1 & 2) in a local MLflow server

5. **Model Serving**  
   - Built a FastAPI app (`/services/api/main.py`) that:  
     - Fetches online features from Feast  
     - Loads the registered MLflow model  
     - Exposes `POST /predict` for real-time PMâ‚‚.â‚… forecasts  
   - Dockerized the API and orchestrated it alongside Airflow, Postgres, and MLflow via Docker-Compose

---

## ğŸ—‚ï¸ Project Structure

â”œâ”€ dags/ # Airflow DAG definitions (ingest_defra, ingest_metoffice)

â”œâ”€ data/

â”‚ â”œâ”€ bronze/ # Raw JSON/CSV from DEFRA & Met Office

â”‚ â””â”€ silver/ # Parquet tables: pollutant_hourly, weather_hourly, joined

â”œâ”€ feature_repo/ # Feast feature store definitions

â”‚ â”œâ”€ feature_store.yaml

â”‚ â””â”€ aq_feature_view.py

â”œâ”€ mlruns/ # MLflow artifact root (models + run logs)

â”œâ”€ mlflow.db # MLflow SQLite backend

â”œâ”€ services/

â”‚ â””â”€ api/

â”‚ â”œâ”€ main.py # FastAPI application

â”‚ â”œâ”€ requirements.txt

â”‚ â””â”€ Dockerfile

â”œâ”€ docker-compose.yml # Dev stack: Postgres, Airflow, MLflow, API

â”œâ”€ train_routeaq_pm25.py # Script/notebook for full train â†’ log â†’ register
 workflow
 
â””â”€ README.md # â† you are here


---

## ğŸš€ How to Run Locally

1. **Prerequisites**  
   - Docker & Docker-Compose  
   - Python 3.10+/venv  

2. **Initialize Airflow & Postgres**  
   docker-compose up -d postgres webserver scheduler airflow-init
Run MLflow

Option A (quick) in your venv:

mlflow server \
  --backend-store-uri sqlite:///mlflow.db \
  --default-artifact-root ./mlruns \
  --host 0.0.0.0 --port 5000
Option B (Docker):
docker-compose up -d mlflow
Trigger Ingestion

Airflow will backfill DEFRA & Met Office DAGs from start_date=2025-06-01

Check /opt/airflow/data/bronze inside the webserver container

Train & Register Model
python train_routeaq_pm25.py
Confirm runs & model versions in MLflow UI at http://localhost:5000

Start the API
docker-compose up -d api
Health check: curl http://localhost:8000/health â†’ {"status":"ok"}

Prediction:
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"site_id":"ABD7","timestamp":"2025-08-05T14:00:00Z"}'
ğŸ”­ My Next Steps
Integration Tests

Write a simple pytest or bash script to automatically hit /predict and assert a valid response.

Model Monitoring

Integrate Evidently in an Airflow DAG to generate drift & performance reports

Configure alerts (e.g. email or Slack) on metric degradation.

Cloud Deployment

Push routeaq-api image to AWS ECR

Deploy Airflow & MLflow on AWS (ECS, EKS, or Cloud Composer + managed services)

Use S3 for Feast offline store & RDS for online store.

CI/CD & Best Practices

Add GitHub Actions for linting (flake8), testing, and building Docker images

Set up pre-commit hooks and a Makefile for common commands

Document environment variables and versions in requirements.txt or environment.yml

Iâ€™m excited to keep refining RouteAQâ€”next up, automated tests and basic monitoring so I can ensure this forecast service runs smoothly in production!